# 2.2e
## `cp`
`cp [source] [target]`
- `cp -n`: prevents overwriting existing files
- `cp -i`: prompt confirmation

## `mv`
`mv [file] [dir]`
- `mv [oldfile] [newfile]`: rename

## `head`, `tail`
`head`/ `tail -5 [file]`

## `echo`
display text after `echo`

`echo sometext > [file.txt]`: overwrite to `file.txt`

## `cat`
Concatanate

- Use overwrite:
`cat [file1] [file2] > [file1]`
- `cat [file1] > [non-existing-file2]`: duplicate file1 to new file
- `cat > [newfile]`: Ask for input, use **ctrl+D** for EOF
- `cat *`: concatanate all files
- `cat n*`: concatanate all files starting with n (similar to wildcards: \*n*)

# 2.12
## HDFS
### Parallel computing
- Shared RAM by partitioning work
- Message passing by partitioning data

### Where HDFS fits:
- Very large file
- Streaming data access: efficient
- Using commodity hardware: cheap
- Low latency
- Lots of small files
- Multiple writers & arbitary file modifications

### Underlyign filesystem opt.
- Commonly Used:
    - **ext3** (2001, Yahoo, ~32TB)
    - **ext4** (2008, Google, ~1EB, fast as XFS)
- Others:
    - **XFS** (1993, Silicon Graphics, parallel I/O, *cant be shrunk*, *slow metadata processing*)

### Blocks
- Min: 512 Bytes
- Filesystem blocks: An integral multiple of the disk block size, typically a few kilobytes
- HDFS blocks: 64MB or **128MB**, files broken into chunks

# 2.14
## Block abstraction:
- Simplifies the storage subsystem 
- Fits well with replication
- `hdfs fsck / -files -blocks`: See blocks5(i)

## Nodes
### Name nodes:
manage file system namespace, maintains the filesystem tree and the metadata (`Fslmage`,`EditLog`)
### Data nodes

## Basic filesystem operations
`hadoop fs`: `hadoop fs -help`, `hadoop fs -mkdir`
- `hadoop fs -copyFromLocal`=`hadoop fs -put`: from local to HDFS
- `hadoop fs -copyToLocal`=`hadoop fs -get`: from HDFS to local
- `-rm`, `-mv`, ...

# 2.16
[OVA](https://archive.cloudera.com/hwx-sandbox/hdp/hdp-2.5.0/HDP_2.5_virtualbox.ova)

### VM Hadoop config. 
50MB VRAM
VMSVGA

Hadoop port: `4200` (`127.0.0.1:4200`)

Username: `root`

Password: `hadoop`

# 2.19
## Block info
`hfds fsck /dir -files -blocks | less`

Compare files (MD5)
`openssl dgst -md5 /dir1/file1 /dir2/file2`

# 2.21
## Hortonworks Sandbox
- Personal portable Hadoop env.
- W/ pre-configured image

### HCatalog
- Table+storage management layer for Hadoop
- Enable w/ diff. processing tools
    - RCFile, CSV, JSON, SequenceFile, ORC, etc.

### Ambari
Open framework for provisioning, managing, and monitoring Apache Hadoop clusters
- Offers a collection of tools and APIs that mask the complexity of Hadoop, simplifying operation of clusters no matter sizes.
 
# 2.23
HDFS port: `50070` or `50075`
### HDFS 2.x features
- NameNode High Availability
  - 2 redundant NameNodes in active/passive config
  - Manual or automated failover(zookeeper)
- NameNode Federation
  - Multiple indep. NameNodes using same collection of DataNodes

## MapReduce
- Programming model: data processing (parellel computing for large data)
- Break processing into 1. Map phase (Map func.) 2. Reduce phase (Reduce func.)
- Each phase has key/value pairs as input and output

### Terms
- M/R job: unit of work that a client want to perform=input data+M/R program+config info. Hadoop runs job by dividing into map task & reduce task
- Jobtracker (on name node): Coordinator, reschedule if certain task failed
- Tasktracker (on data node): Run tasks and send progress to jobtracker
- Input split: 64 or 128 MB to M/R job, one map task for each split

# 3.1
`cat filename | python ./map.py`: use `filename` as stdio in `map.py`
&rightarrow; `cat filename | python ./prog.py | python ./reduce.py`: use output from map func. and pass into reduce func., get output

# 3.6
## Apache Hive
- Use SQL (HQL)
- Hive stores metadata in an embedded Apache Derby database (relational database)
- Itself is not a relational database, not a full database, no `insert`, `delete`, etc. command
- 4 file format supported: TEXTFILE, SEQUENCEFILE, ORC, RCFILE
- Runs on Hadoop, which is a batch-processing system.
- Can have latency (not for small dataset)
- Supports: 
  - Ad hoc queries
  - Summarization
  - Data analysis

# 3.8
## Hive
`hive -f myquery.hive`: use `flag` to specify a file contains hive script

### HiveQL
- Based on SQL-92
- Support multiple dialects
- Convert HQL queries to MapReduce jobs
- Support plugging custom MapReduce scripts into queries through `TRANSFORM`

#### Data Types & File Format
- TINYINT, SMALLINT, INT, BIGINT, BOOLEAN, FLOAT, DOUBLE, STRING, TIMESTAMP, BINARY
- Extra 3 collection data type
  - STRUCT: Fields can be accessed using `.`: `Person.Name`
  - MAP: Collection of key-value tuples. Fields accessed using `[]`: `Salary[Name]`
  - ARRAY: Fields accessed using indices

- E.g.
  ```sql
  create table t(
    name string
    salary float
    subord array<string>,
    deduct map<string,float>,
    addr struct<street:string,city:string, zip:int>
  )
  ```

#### Textfile encoding of data values
- `^A; \001`: Separate all fields (columns)
- `^B; \002`: Separate elements in ARRAY or STRUCT, or key-value pair in MAP
- `^C; \003`: Separate key-value pair in MAP.
*`^A`=`Ctrl+V`+`Ctrl+A` in vim*

# 3.11
### Override:
```
row format delimited 
fields terminated by '^'
...
stored as textfile
```

### Data def. Language (DDL)
- Hive adds extensions to provide better performance in the context 
```
hive> create database if not exists dbname
      (location 'dir');

hive> desrcibe database dbname;
hive> desrcibe database default;

hive> show database

hive> 
```
#### Dropping
```
drop database if exists dbname (cascade);
```
#### create table
```
create table if not exists tname(
)
comment 'something'
tblproperties ('creator'='somebody','created_at'='sometime')
```
- `like` to create table w/ same schema is enabled (Oracle &times; MySQL &#10003;)

#### More `describe`
- `describe extend`
- `describe formatted`

### Managed tables & external tables

# 3.13
how hive stores table:
```
/employees/country=CA/state=AB
/employees/country=CA/state=BC
...
```

### Show partitions:
 `show partitions tabname <(parition colname = "restrain")> <log_messages>`

### Drop table
Managed table: Metadata is also dropped

### Alter table
- Change metadata, not data itself
- Ensure any modification are consistent with actual data
#### Rename
`alter table tab rename to newname`

#### Add
`partition(year=2011, month=1, day=1) location '/logs/2011/1/1'`

#### Prevent/enable dropping
`alter table tabname partition( ) enable/disabel no_drop / offline`

### Load CSV
```
create table tabname ( )
row format delimited 
fields terminated by `,`
lines terminated by '\n';
```

#### Load table from centos to hadoop (hive)
```
load data local inpath '/file.txt' overwrite into table tabname
```

# 3.15
### Load CSV
```
load data local inpath 'dir' overwrite into table tabname <partition(country='us',state='IL')>
```

`local`:
- Used: path is in local, data is copied into final location
- Omitted: path is in HDFSï¼Œ data is moved from path into final location

`insert into`: Append to existing table

`insert overwrite`: Overwrite, always use with `select *` query to create similar tables

# 3.20
### `like` and `rlike`
- `like`: wildcards (`%x`/`_x`, `x%`, `%x%`: ends/one exactly before, begins, contains; )
- `rlike`: Use Java **r**egex

### Extra 
- `left semi join`: return records from left table if records are find in right table on `on` condition

- `map-side joins`

- `sort by`: faster
- `distribute by`: controls how map output is divided among reducers, **must before `sort by`**

- `cluster by`: `distribute by`+`sort by`

# 3.22
## Views and UDF in Hive
- Views: Logical construct for simplify queries
- UDF: Allow users to extend HiveQL
  - Must use `transform`
  - `sys` package must be imported
  - Must be used in Hadoop streaming
### Use UDF:
In hive:
```sql
add file "dir"

select transform (op_1, ..., op_n) 
using 'dir' 
as alias 
from tabname;
```

# 3.29
## Apache Pig
- Scripting lang. developed initially at Yahoo
- Uses both HDFS and mapReduce
- Can be used as a component to build large and complex app. for business soln.
- Not verbose as Java
- Good for ETL, create a long series of steps to place the data into a more accessible form.
- Keyword case and UDF names: insensitive, fields and relation: case sensitive
- Wont start executing PigLatin entered until `store` or `dump` executed.

### Pig's 3 Elements
1. **Pig Latin**
   - Keywords not case-sensitive
   - Scripting lang. of Pig, a **dataflow lang.**
   - Requires **no metadata or schema**
   - Statements are translated into series of M/R jobs
2. **Grunt**: Interactive shell of Pig
3. **Piggybank**: Shared repo. of UDFs

# 4.8
### HDFS in Pig
Following commands do not need `fs -`: 
- `cat filename`
- `copyFromLocal localfile hdfsfile`
- `rm -r filename`

### Controlling Pig from Grunt
- `kill jobID`
- `exec`
- `run`: same as `exec`, but import all relations

### Pig's Data Model
- Scalar tpyes:
  - int: 4B
  - long: `5000000000L`
  - float: 4B
  - double: 8B
  - chararray: str
  - bytearray: BLOB or AoB
- Complex types: 
  - map: `['key1'#'value1',...,'key2'#'value2']`
  - tuple: `(val1,val2,...,valn)`
  - bag: an **unordered** collection of tuples
- Nulls:
  - x+null=null, for all x

### Pig's Schemas
- If exists, pig uses it. If no, pig guesses.
- Specify when loading: `dividends = load 'dividents' as (exchange: chararray, symbol:chararray, date: chararray, dividend:float);`
  - Pig truncates extras for more than 4 fields.
  - Pig pads with null for less than 4 fields.
- If no given explicit data types, **bytearray** is assumed.

### Data type casting
- explictly:
`(int)bat#'bateOnBalls'-(int)bat#'ibbs'`

- implicitly:
  1. loading `as`
  2. math operation: `a*b` -> convert to `float` (**widening**)
     - int, long: long
     - int, long, float: float
     - int, ling, float, double: double
  - No casting between numeric and char. 