# 1.24
## Mistake-bound model
- Get an image
- Output cat or not
- See true label
- Update alg.+repair
## Consistent 
$S'=\{\bar{W^1}, \cdots, \bar{W^H} \}$. In each period t, see an image I, pick $\forall \bar{W} \in S^t$, output $f(I,\bar{W})$. Right: $S^{t+1}=S^t$, o.w. $S^{t+1}=S^t \backslash \bar{W}$

# 1.26
## Logistics
- Search academic lit. to id. an advanced topic/problem/app. 
- Form a team of 1~3. Upload to canvas a 1-page doc of members and 1/2 page for topic.
- **Due Feb.16**

## Cont. (consistent)
**Argument**: Everytime made a mistake, throw away $\bar{W}$, only H $\bar{W}$'s. $\Rightarrow$ At most H mistakes.

## Attempt
$S'=\{\bar{W^1}, \cdots, \bar{W^H} \}$. In each period t, see an image I, pick $\forall \bar{W} \in S^t$, output $f(I,\bar{W})$. But in addition evaluate all in $S^t$, throw away all the remaining.


## Halving alg.
$S'=\{\bar{W^1}, \cdots, \bar{W^H} \}$. In each period t, see an image I, evaluate $f(I,\bar{W})$;if majority is cat: cat; if majority is not cat: not cat. If correct: $S^{t+1}=S^t$, if wrong remove all incorrect $\bar{W}$.

- $|S^{t+1}| \leq \frac{1}{2} |S^t|$
- max # of mistake=$\log_2{H}$

## Perceptron alg.
### Happy or sad
Suppose there are n possible types of obj.: $O_1, \cdots, O_n$. Each obj. is either happy or sad. $O$ is happy if it has more happy obj. than sad obj.. Suppose each image is preprocessed into $(x_1,\cdots,x_n)$ and $x_i$ is 1 if **at least one** $O_i$ is in that image and 0 o/w.

# 1.31
## Perceptron alg. (cont.)
$I \rightarrow (x_1,\cdots,x_n),x_i$ stands for the existence of object $i$ 

1. see an image vector
2. output happy/sad 
3. update its beliefs/alg.

## Margin
Problem gets harder as they get closer to the boundary (degeneracy)

## Cauchy-Schwarz Ineq.
$$\left|\sum_{i=1}^n x_i  y_i\right| = |\langle x,y \rangle |\leq  ||x||\text{ }||y||$$

# 2.2
Argument: $\forall x_1, \cdots ,x_n, y_1, \cdots, y_n,z$:
1. $\sum_{i=1}^n(zx_i+y_i)^2 \geq 0$ 
2. $(\sum_{i=1}^n x_i^2)z^2 + 2(\sum_{i=1}^n x_iy_i)z+ \sum_{i=1}^n y_i^2 \geq 0$ (1.) 
3. $z^2 + \frac{2(\sum_{i=1}^n x_iy_i)z }{\sum_{i=1}^n x_i^2}+ \frac{\sum_{i=1}^n y_i^2}{\sum_{i=1}^n x_i^2}  \geq 0 \Rightarrow z*=-\frac{2(\sum_{i=1}^n x_iy_i) }{\sum_{i=1}^n x_i^2}\Rightarrow$
Min val=$-\left(\frac{(\sum_{i=1}^n x_iy_i) }{\sum_{i=1}^n x_i^2}\right)^2 + \frac{\sum_{i=1}^n y_i^2}{\sum_{i=1}^n x_i^2} \geq 0$ (above) $\Rightarrow \left( \sum_{i=1}^n x_iy_i \right)^2 \leq \sum_{i=1}^n x_i^2 \sum_{i=1}^n y_i^2 \Rightarrow$ C-S Ineq.


## Perceptron (cont.)
$W'=0$
At time $t$, see $X$ ($X_k=1$ if object $k$ is in $I_t$), evaluate $W_tX$
If $\geq 0$: happy; else: sad
If said happy, really sad: $W_{t+1}=W_t-X$
If said sad, really happy: $W_{t+1}=W_t+X$
If predict right: pass

**Claim.**
Perceptron makes at most $\frac{1}{\gamma^2}$ mistakes. 
*$\gamma$: All happy image have at least $\gamma n$ more happy than sad; all sad images have at most $-\gamma n$ more sad than happy.*

# 2.7
Whenever made an mistake: 
1. $\sum (W_t)^2$ increases significantly $\sum (W_{t+1})^2 >> \sum (W_t)^2$ 
2. But $\sum (W_t)^2$  cant get too big

1+2: few mistakes potential func.

Claim.
$||W_t||^2 \geq \frac{1}{n} (W^* \cdot W_t)^2$

Pf:
$$
\begin{align}
|W^* W_t| & \leq ||W_t|| \cdot ||W^*||\\
|W^* W_t| & \leq ||W_t|| \sqrt{n}\\
\frac{|W^* W_t|}{\sqrt{n}} &\leq ||W_t||
\end{align}
$$

$W_{t+1}W^* - W_tW^*=(W_t-x)W^*-W_tW^*=-xW^* 
\geq \gamma n$
For every mistake, $W_tW^*$ increases by at least $\gamma n$, therefore, after m mistakes, the potential function $||W_t||^2 \geq m^2 \gamma^2 n$

# 2.9
cont.
$$\sum_{i=1}^n W_{t+1}^2=\sum_{i=1}^n (W_{t}-x_i)^2=\\\sum_{i=1}^n ||W_t||^2-2 \sum_{i=1}^n W_{t}x_i^2+ \sum_{i=1}^n x_i^2 \leq mn$$

## Winnow
$W_0=\mathbf{1}$
Test: $W_t x \geq n$ if yes: happy, o/w: no.
If said happy really sad, half weights of all obj. in the image.
If said sad really happy, double weights of all obj. in the image.
$$\sum_{i \in HAP^* } x_i \geq 1$$

# 2.14
### S-H mistake (said sad, really happy)
**Claim**: #S-H mistake $<r(\ln (n)+1)$
1. S-H mistake: some ${W_t}_i$ for $i \in HAP^*$ doubles
2. The weight of any truly happy obj. never decreases: ${W_t}_i$ increases  for $i \in HAP^*$ 
3. No weight ever exceed $2n$
4. $|HAP^*|=r$
-> S-H mistake $<r(2\ln (n))$

5.  $W^{'}_{i}=W_i +  W_ix_i \Rightarrow \sum W^{'}_{i}= \sum W_i +\sum W_ix_i \Rightarrow \Delta \sum W_i \leq n $

### H-S mistake (said happy, really sad)
**Claim**: H-S mistake: weight sum decreases by at least $\frac{n}{2}$
1. $\sum_{i} {W_t}_i x_i \leq n$
2. Halving weights: $W^{'}_{i}=W_i - 1/2 W_ix_i \Rightarrow \sum W^{'}_{i}= \sum W_i -  \sum 1/2 W_ix_i$


# 2.16
### Winnow (cont.)
\[0 \leq \sum w_i \leq n+ n m_{SH}- \frac{n}{2} m_{HS}\\
\Rightarrow m_{HS} \leq 2+ 2 m_{SH}\\\Rightarrow m_{Tot} \leq 2+ 3 m_{SH} = 2 +3r(\log_2 {n}+1)\]
**Winnow makes at most $2 +3r(\log_2 {n}+1)$ mistakes.**

## The Experts Problem 
### Weighted majority 
n experts, $W_0=\mathbf{1}$
At time $t$, see predictions $Z_1, \cdots, Z_n$ of n experts.
Evaluate $\sum_{i:z_i=1} w_i -\sum_{j:z_j=0} w_j :$ If $ \geq 0 \rightarrow 1$, if $\leq 0 \rightarrow 0$
For wrong prediction $z_i$, $W^{'} _ i \leftarrow \frac{1}{2}W_i$

# 2.21
## The Experts Problem 
Claim: $m \leq 2.5 m^* +2.5 \ln(n)$

Subclaim 1:
$$\sum_{z_i \neq z^*} w_i \geq \frac{1}{2} \sum w_i (1)\\
\sum_{z_i \neq z^*} w_i + \sum_{z_i =z^*} w_i= \sum w_i$$

Subclaim 2:
$$w_i' = w_i - \frac{1}{2} w_i \times I(z_i =z^*)\\
\sum w_i' = \sum w_i - \frac{1}{2} \sum w_i \times I(z_i =z^*)\\
\Rightarrow \sum w_i'\leq \frac{3}{4} \sum w_i \text{ (using (1))}$$


At the end of year:
$$\sum w_i \leq \left(\frac{3}{4}\right)^m \cdot n=result \cdot \sum w_i$$
Also, $\sum w_i \geq \frac{1}{2} m^*$

$\Rightarrow \left(\frac{3}{4}\right)^m \cdot n \geq \frac{1}{2} m^*\\
m \leq \lg(m^*+ \lg (n)) \\
m \leq 2.5 m^* +2.5 \lg(n)
$

# 2.22 Meeting
1. Flower Indetification
   1. Research
   2. Run
   3. Contrast(expert problem)



# 2.23
## Follow The Leader
- breakties to $E_1$

|E1|E2|Guess|Truth|Mistake E1|Mistake E1|Mistake ALg (ME1+ME2)|
|-|-|-|-|-|-|-|
|1|0|1|0|1|0|1|
|0|1|1|0|0|1|2|
|1|0|1|0|2|1|3|
|0|1|1|0|2|2|4|
|1|0|1|0|3|2|5|
|0|1|1|0|3|3|6|
|1|0|1|0|4|3|7|
|0|1|1|0|4|4|8|

For case above, $m=2m^*$.

## Weighted majority (modified)
n experts, $W_0=\mathbf{1}$
At time $t$, see predictions $Z_1, \cdots, Z_n$ of n experts.
Randomly select one expert:
with probability: $w_i/\sum w_i$
Evaluate $\sum_{i:z_i=1} w_i -\sum_{j:z_j=0} w_j :$ If $ \geq 0 \rightarrow 1$, if $\leq 0 \rightarrow 0$
For wrong prediction $z_i$, $W^{'} _ i \leftarrow (1-\epsilon) W_i$

Claim: $\mathbb{E}(m) \leq (1+\epsilon) m^* + \frac{\ln(n)}{\epsilon}$
$F_t = \mathbb{P}(mistake|t)=\frac{\sum_{i: z_i \neq z^*}w_i}{\sum w_i}$

Pf:
$$\mathbb{E} = \sum_{t} F_t$$
$$w^{'}=w-\epsilon I(z_i \neq z^*)w\\
\sum w^{'}=\sum w-\epsilon \sum_{z_i \neq z^*}w \cdots (1)\\
$$

$$\sum_{z_i \neq z^*}w = F_t \sum w\cdots (2)$$

$$(1)+(2): \sum w^{'}=\sum w-\epsilon F_t \sum w= (1-\epsilon F_t) \sum w$$

At the end: $n\prod_{i=1}^T (1-\epsilon F_i)$

# 3.6
## General Expert Problem
- n experts $e_1, \cdots e_n$
- At time t, select an expert observe loss $l^t_1, \cdots l^t_n,$ loss can be +/-
- **Regret**: $\mathbb{E}\left[\sum_{i=1}^T l^t_{i(t)}\right]-\min_j \sum^T_{t=1}l^T_j$ (total loss over time T of **each** website)

## Hedge
#### Algo.
- $W_0=\mathbf{1}$
- At time t, select expert i with proportional to $w^t_i/\sum w^t$. Observe $l_1^t, \cdots, l_n^t \in [-1,1] , \forall i \in [1,n]$
- For $i=1,\cdots, n$, $W'_i=(1-\epsilon l^t_i)W_i$ &Rightarrow; loss &prop; penalty 

Let $L^*:= \min \sum^T_{i=1}l^i_.$ be the total loss for best expert; $L:=\sum^T_{i=1}l^i_.$

Subclaim 1:
$$\sum_i W'_i= (1-\epsilon \mathbb{E}[\hat{L_t}]) \sum_i w_i$$

Pf:
$$\frac{\sum_{i}W'_i}{\sum_j W_j}=1-\epsilon \sum_{i} l_i \frac{w_i}{\sum_j W_j}\\
\frac{\sum_{i}W'_i}{\sum_j W_j}=1-\epsilon \sum_{i} l_i \mathbb{P}(l_i)\\
\frac{\sum_{i}W'_i}{\sum_j W_j}=1-\epsilon \mathbb{E}(l_i)
$$

Subclaim 2:
$$\sum_i W^T_i \geq \prod_{i=1}^T (1-\epsilon l^i _{*})$$

$\ln(1-x)\geq -x-x^2$

$$\geq e^{-\epsilon L* -\epsilon^2 \sum_{t=1}^T (l^T_*)^2} \geq e^{ \epsilon L* -\epsilon^2 T}$$

&Rightarrow; $E[L] \leq L^*-\epsilon T+ \frac{\ln(n)}{\epsilon} \rightsquigarrow \text{Regret} \leq 2 \sqrt{\ln(n)} \sqrt{T}$

### Gen Exp Max variant
- At time t select an expert and get its reward observe rewards of all expert.  

# 3.8
First day get \$1. Each day choose how much to invest ($D$, can be a fraction), for probability $p$ you get $2D$, for $1-p$ you get $0$. 

d: investment on D1
V(d): input start of D2, output how much of to invest for D2 (% of d)
D2: total money we have at start of D2

$$
\mathbb{E}(I_2|up,up)=2V(D_2)+D_2-V(D_2)=V(d+1)+d+1\\
\mathbb{E}(I_2|up,down)=0\times V(D_2)+D_2-V(D_2)=d+1-V(d+1)\\
\mathbb{E}(I_2|down,up)=V(1-d)+1-d\\
\mathbb{E}(I_2|down,down)=1-d-V(1-d)\\
\Rightarrow \mathbb{E}(I_2)=p(2p-1)V(d-1)+(1-p)(2p-1)V(1-d)+(2p-1)d+1
$$
&Rightarrow; If $p \geq 1/2: V(d+1)=d+1; V(1-d)=1-d \Rightarrow \argmax_d 2p(2p-1)d+p(2p-1)+(1-p)(2p-1)+1 \Rightarrow$
All-in, if $p< 1/2$, hold

# 3.13
## Invest fraction $f$:
### Claim. 
If $p>1/2, f\neq1$, guaranteed with probability of 1 for the long-term exponential growth.

Let 
$$
Z_t\in R.V. = \begin{cases}
2: p\\
0: 1-p
\end{cases}
$$

$D_t$: money at start of day $t
, D_1=1$

$\Rightarrow D_{t+1}=Z_t(fD_t)+(1-f)D_t=(Z_tf+1-f)D_t$

$$
\begin{align} 
D_{T+1}&=\prod _{t=1}^T (Z_tf+1-f)\\
&=\exp\left(\ln \prod _{t=1}^T (Z_tf+1-f)\right)\\
&=\exp\left[ \sum _{t=1}^T\left( \ln  (Z_tf+1-f)/T \right) \cdot  T\right]\\
&= \exp(\mathbb{E}(\cdot)T) 
\end{align}
$$
Note 
$$
\ln  (Z_tf+1-f) \in R.V. = \begin{cases}
\ln(1+f): p\\
\ln(1-f): 1-p,
\end{cases}
$$
which gives $\mathbb{E}(\cdot)=p\ln(1+f)+(1-p)\ln(1-f)$

$(7)$ becomes: 
$$
\begin{align}
&=\exp(T(p\ln(1+f)+(1-p)\ln(1-f)))\\
&=\exp(p\ln(1+f)+(1-p)\ln(1-f))^T \\
&=\left[ (1+f)^p+(1-f)^{1-p}\right]^T
\end{align}
$$

-- Kelly gambling/criterion

#### Optimization:
$$
\begin{align}
\argmax_f \left[ (1+f)^p+(1-f)^{1-p}\right]^T\\
=\argmax_f (1+f)^p+(1-f)^{1-p}\\
=\argmax_f  p\ln (1+f)(1-f)^{1-p}
\end{align}
$$

# 3.15
$D_{t+1}=fD_tX_t+(1-f)D_tY_t=(fX_t+(1-f)Y_t)D_t$
&Rightarrow;$\prod_{t=1}^T fX_t+(1-f)Y_t$

### ~Expert's problem:
Map $f$ to experts:
1. Infinity number of experts &Rightarrow; $N=1/f$
2. Product, not sum &Rightarrow; take log

$$\argmax_i \sum_{t=1}^T \ln(f_iX_t+(1-f_i)Y_t)$$